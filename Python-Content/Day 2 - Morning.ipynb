{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a07488-7854-49c1-8262-661921321626",
   "metadata": {},
   "source": [
    "# Day 2 - Morning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0bba3-d59f-4905-8f34-82ea764f4703",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5092a9c-46ee-4d6f-ad7e-de05dbe9cd4d",
   "metadata": {},
   "source": [
    "In this section of the course we will discuss Deep Learning. \n",
    "\n",
    "Deep Learning is fundamentally different to classical machine learning in a few ways. First, it is normally applied to non-tabular data. We mostly use deep learning for image data, \n",
    "\n",
    "Also, in Deep Learning, you are normally not very interested in feature engineering. That is to say, we generally do not select features or perform feature engineering - we in fact allow the network to learn which features are important all by itself. \n",
    "\n",
    "Normally, in the problems we discussed yesterday, we carefully selected features, removed features, or acquired datasets where features were curated in order for \n",
    "\n",
    "I will illustrate this with a demo:\n",
    "\n",
    "<https://playground.tensorflow.org>\n",
    "\n",
    "In the playground, we will\n",
    "\n",
    "1. Mimick feature engineering (all except $\\sin$) and train a network using these features and 8 neurons on the swiss roll data\n",
    "2. Remove all engineered features and instead use multiple layers, achieving the same thing (ensure the use of ReLU activation function)\n",
    "\n",
    "As we saw, adding layers upon layers allows the network to learn very complex functions. \n",
    "\n",
    "We will look at an example of MNIST (Modified National Institute of Standards and Technology) dataset demonstrates this very well. \n",
    "\n",
    "Imagine you had a house price dataset. Well the features are quite easy to understand, and you could probably easily remove features that were not important, such as house colour, or street number. \n",
    "\n",
    "This also means there is far less domain knowledge required to train neural networks. You might not neccessarily need to know the details of your data in order to train something - for example in medicine, you might not have to know the details of the RNA sequence data that you wish to classify. \n",
    "\n",
    "## Why *Deep* Learning?\n",
    "\n",
    "So, neural networks have been around for decades, and for many years were actually ignored. It is only recently that we have seen a resurgance of interest in neural networks and specifically the rise in **deep** learning. \n",
    "\n",
    "This has happened for 2 main reasons\n",
    "\n",
    "1. We have become very good at gathering lots of data\n",
    "2. Hardware has become very powerful at training neural networks\n",
    "\n",
    "This has led to networks getting larger and larger, meaning they can perform ever more complex tasks. In many fields, such as image classificaiton, they are in a class of their own in terms of capabilities, as is the case with machine translation, image generation, image segmentation, and so on.\n",
    "\n",
    "This brings up a further point: hardware. Neural networks require lots of computing power to train, and it turns out that the devices that are best suited for performing the calculations required to train a neural network are in fact GPUs (graphics processing units) and not CPUs. Most computers, especially laptops, do not have a dedicated GPU that is powerful enough to train neural networks. However, it is still possible to train networks using a PC's CPU, it is magnitudes slower.\n",
    "\n",
    "However, for smaller datasets, as we will see here, training can be performed on a CPU. For example training a small network that has to classify small images can be trained on a normal PC or laptop. \n",
    "\n",
    "Second, there are ways to get a GPU for free to perform training tasks. For example, Google Colab offers GPUs for free, and I will demo this later.\n",
    "\n",
    "### Areas Where Deep Learning is Used\n",
    "\n",
    "Neural Networks and Deep Learning algorithms can be applied to nearly any type of problem, but they are more suited to some problems than others. For example, they tend not to be used for tabular/text data, such as those we discussed yesterday, where algorithms like random forests can still outperform them. See for example the paper: Grinsztajn L, Oyallon E, Varoquaux G. Why do tree-based models still outperform deep learning on tabular data? arXiv: <http://arxiv.org/abs/2207.08815>\n",
    "\n",
    "So in this section we will concentrate on deep learning for image analysis, and specifically image classification. In medicine, deep learning is mostly used for image classification or image segmentation. In this seminar, we will demo image classification, but we will also discuss how segmentation is performed.  \n",
    "\n",
    "Here are some of the fields and areas where deep learning is very successful\n",
    "\n",
    "- NLP: In natural language processing (NLP), networks have been trained at answering questions, speech recognition, document summarisation, document classification \n",
    "- Computer vision: Satellite imagery interpretation, face recognition, image captioning, reading traffic signs, highlighting and recognising pedestrians and vehicles in autonomous vehicles \n",
    "- Medicine:: Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopath- y\n",
    "- Biology:: protein folding (DeepMind), classifying proteins, genomics tasks, protein/protein interactions\n",
    "- Image generation: Colorising images, increasing image resolution (super resolution), de-noising/removing noise from images, converting images to art in the style of famous artists\n",
    "- Recommendation systems: product recommendations, film recommendations, music recommendations\n",
    "- Playing games: Chess, Go, Atari video games, and many real-time strategy games\n",
    "- Robotics: Boston Dyamics robots that can run, climb, and so on.\n",
    "\n",
    "(Some of the points above from: <https://course.fast.ai> which provides some very good introductory material on Deep Learning.)\n",
    "\n",
    "Also see this dataset: <https://www.robots.ox.ac.uk/~vgg/data/pets/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc3e3f-bd85-4e99-bed7-0264c73ee883",
   "metadata": {},
   "source": [
    "## Funny Joke\n",
    "\n",
    "Deep learning has moved very fast in the past few years. Here is an example from the comic xkcd:\n",
    "\n",
    "<img src=\"./img/task.png\" width=\"400\"/>\n",
    "\n",
    "*Source*: <https://xkcd.com/1425/>\n",
    "\n",
    "This comic is not particulary old, and the text suggests that to build a system that can recognise birds would be 'virtually impossible', yet what we will see now is that in fact, we can perform a classification of this type very quickly, and we will do so during this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d2509-ef61-4c17-8358-5dddc9b6aa91",
   "metadata": {},
   "source": [
    "## How Neural Networks Work\n",
    "\n",
    "Here we will describe briefly the concepts behind neural networks. \n",
    "\n",
    "They are called neural networks because they are based on the idea of neurons and loosely mimick the neurons in the (human) brain.\n",
    "\n",
    "![neuron](./img/chapter7_neuron.png)\n",
    "\n",
    "*Source*: <https://colab.research.google.com/github/fastai/fastbook/blob/master/01_intro.ipynb>\n",
    "\n",
    "The neuron in neural network is based loosely on the biological neuron. In a neural network, we have a lot of interconnected neurons organised in layers, where neurons can fire or not fire based on their inputs and a weight. The weight associated with the neuron controls if it fires or not, and these weights are adjusted (or learned) during the training of the network. Ultimately, a network's ability to perform some function is based on the firing of these neurons. The neuron fires based on a threshold which is controlled by something called an activation function. By 'fires' we mean it outputs something (a value). The threshold is controlled using a weight. These weights are what are learned when the network is training. So what the network learns, is which neurons should fire and which should not.\n",
    "\n",
    "These weights are updated when you train the network. During training you pass data through the network, and the weights are updated based on the error or loss of the network. If the error is very high for a given prediction during training, these weights are nudged more than if the error was very small.\n",
    "\n",
    "Hopefully you now have some understanding how neural networks work, and how they are trained. Check the TensorFlow Playground website again if you wish to visually "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6ec6e-1946-4a7e-a447-b55cb3425f4d",
   "metadata": {},
   "source": [
    "---\n",
    "# PyTorch\n",
    "\n",
    "There are a number of frameworks available for neural network development:\n",
    "\n",
    "- PyTorch (which we will use during this course), from Facebook\n",
    "- TensorFlow, from Google\n",
    "- Keras (front-end for TensorFlow and PyTorch, higher level API)\n",
    "- FastAI (high level API, good for quick experimentation, and getting a model trained quickly and easily)\n",
    "\n",
    "As mentioned, we will be using Torch during this course. Torch is probably the most popular framework for deep learning currently. \n",
    "\n",
    "Unfortunately, traing neural networks require a lot of boilerplate code in order to get running. \n",
    "\n",
    "However, there are easier approaches and I will demonstrate these after we have covered how a network is trained generally.\n",
    "\n",
    "First some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188520e-a965-4515-b5fd-9acd89d20707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe1306f-77c8-43ac-aa8a-51861b5dd275",
   "metadata": {},
   "source": [
    "In this example, we are going to train a simple neural network to learn how to classify hand-written digits. \n",
    "\n",
    "MNIST is included in PyTorch, but there are many datasets to choose from, see the PyTorch documentation for details: <https://pytorch.org/vision/stable/datasets.html>\n",
    "\n",
    "The MNIST dataset consists for 60,000 training images and 10,000 testing images. They are only 28x28 pixels in size, meaning it is a very popular dataset for benchmarking and demonstrating neural networks.\n",
    "\n",
    "![MNIST](./img/MnistExamplesModified.png)\n",
    "\n",
    "By Suvanjanprasai - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=132282871\n",
    "\n",
    "So, the aim of this task is to train a neural network to recognise the digits. In other words, once trained, the network should be able to predict the digit contained in an image passed to the network.\n",
    "\n",
    "Learning to recognise digits is a **supervised** machine learning task: the vast majority of neural network based trained is supervised. Neural networks can be used for both classification and regression. In the case of MNIST, it is a supervised, multi-class classification.\n",
    "\n",
    "Training a neural network in PyTorch is a relatively complex task. However, later we will discuss much easier ways to do this.\n",
    "\n",
    "For now, however, we will demonstrate how this is done by first defining a neural network, training it over a number of **epochs** and then testing it on some unseen data. An epoch is one pass through \n",
    "\n",
    "The general procedure for training a neural network is as follows:\n",
    "\n",
    "- Define your dataset, and any transformations you wish to perform on the data\n",
    "- Define the neural network structure itself. This requires piecing together the layers and define the network's shape, and so on\n",
    "- Define the network's optimiser. You might remember from yesterday that algorithms in Sci-Kit Learn do not normally require you to define the optimisation algorithm. This is not true for PyTorch, as an optimiser must be defined explicitly.\n",
    "- Define the network's loss\n",
    "- Train the network: you train it over a number of epochs that you decide on\n",
    "\n",
    "Note: this is the procedure when you want to train a network from scratch. There are easier options, and we will see these later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0cf57a-3af5-4563-a1d7-74e71dc2c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Set download to true if the data is not already downloaded.\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c1e2b4-8f0d-4139-bf70-2f7208961914",
   "metadata": {},
   "source": [
    "## Creating a Simple Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12157d50-fc29-4f32-9d75-643d31367caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Neural Network Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 500)  # Size must match the input\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()   \n",
    "\n",
    "torchinfo.summary(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f429544-9f72-4b9f-ae1a-c893e1418c12",
   "metadata": {},
   "source": [
    "## Define Loss and Optimizer\n",
    "\n",
    "Define Loss Function and Optimizer: We'll use a common loss function for classification tasks (like Cross Entropy Loss) and an optimizer (like SGD or Adam) to adjust the weights of our network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b09c4f-28a4-42d3-bc03-b7665e3573d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350145ec-7802-47a8-b596-ee7482f11c27",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c30beb-0de7-4e93-abf9-d206438aeb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, optimizer, criterion, epochs=5):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "train(net, train_loader, optimizer, criterion, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf5d5d-3c84-45d1-9b53-16ba0ac651e2",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10bd8cb-4b1b-49cc-94b6-f5cb69326fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674a605-b334-43d0-91ab-2d89bda15d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(net, test_loader)\n",
    "print(f\"Model accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85644ec5-9c12-4a4a-a7d5-662121d305ec",
   "metadata": {},
   "source": [
    "Later, we will see how to evaluate neural networks more carefully.\n",
    "\n",
    "Show some of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8856f2-df69-4c58-9262-21aea7318a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(test_loader)\n",
    "\n",
    "# Get the next batch from the iterator\n",
    "#images, labels = next(iterator)\n",
    "\n",
    "# Or loop over the first 1 batches: (use for images,labels in iterator: for all of them)\n",
    "for images,labels in itertools.islice(iterator, 1):\n",
    "\n",
    "    for image, label in zip(images, labels):\n",
    "        image_for_pred = image.unsqueeze(0)  # Add a batch dimension\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = net(image_for_pred)\n",
    "            predicted_label = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        # Show false positives by saying 'is not; here\n",
    "        if predicted_label.item() is label.item():\n",
    "            plt.imshow(image.squeeze(), cmap='gray')\n",
    "            plt.title(f'Actual Label: {label}')\n",
    "            plt.show()\n",
    "            print(f'Predicted Label: {predicted_label.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e8787-0271-4aa1-a03a-d2d229c7a004",
   "metadata": {},
   "source": [
    "## Other Evaluation Metrics\n",
    "\n",
    "Neural networks are quite different in how you evaluate them. \n",
    "\n",
    "What we saw above was just the accuracy metric. But to truly understand how well they are training, normally we want their losses and accuracies on the training and test sets as we train. \n",
    "\n",
    "For example:\n",
    "\n",
    "![Loss-Acc](./img/loss-acc.png)\n",
    "\n",
    "Normally, you would plot the losses and accuracies for each epoch, on both the training and test set, to watch if they diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eba564-542b-4089-8d2e-cdec63415848",
   "metadata": {},
   "source": [
    "# CIFAR10\n",
    "\n",
    "Let's move on to a more difficult example. Here we will classify between 10 different classes. The images are also colour. And they should be more difficult to classify. Also we will see more classification metrics.\n",
    "\n",
    "For this tutorial, we will use the CIFAR10 dataset. It has the classes: 'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', and 'truck'. The images in CIFAR10 are of size 3x32x32, in other words 3 channel (RGB) images, 32x32 pixels in size.\n",
    "\n",
    "Here are some examples of the images:\n",
    "\n",
    "![CIFAR-Examples](./img/cifar10.png)\n",
    "\n",
    "Let's get the data, which we will get using Torch's `datasets` module, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d4b11-b7d9-4eea-aad3-f3824c5ab59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchinfo\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# These are transformations applied to every image in the dataset\n",
    "# They can include image resizing, coverting to greyscale, and so on\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731398e1-b624-4936-bdce-1c6f1b1af264",
   "metadata": {},
   "source": [
    "One thing that always makes sense is to take a look at the distribution of the data, first with the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aacb677-972a-4d12-89e8-57ab91190880",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(trainset.targets, return_counts=True)\n",
    "pd.DataFrame(np.asarray((unique, counts)).T, columns=['class id', 'count'], index=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c09967-7f93-4ba1-95ed-9f3f4530a122",
   "metadata": {},
   "source": [
    "And the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7fcb5-cb99-4cf4-8225-af649633dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(testset.targets, return_counts=True)\n",
    "pd.DataFrame(np.asarray((unique, counts)).T, columns=['class id', 'count'], index=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df140c1e-9e3c-4cef-a1e7-16925662ab72",
   "metadata": {},
   "source": [
    "We can preview some of the images from the dataset and also see their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d2790-60df-42b3-8719-dc8a5c5ec5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "data_iterator = iter(trainloader)\n",
    "images, labels = next(data_iterator)\n",
    "\n",
    "# show images\n",
    "imshow(make_grid(images))\n",
    "\n",
    "# print labels\n",
    "labels_t = [f'{classes[labels[x]]}' for x in range(batch_size)]\n",
    "print(labels_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a24c9-88dd-43ba-ac1c-15b845dd6d12",
   "metadata": {},
   "source": [
    "Let's now define a network. \n",
    "\n",
    "This is a more complex network that the one above, as we will see when we print its structure and see how many parameters it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ce601-96b1-4549-be85-e5e1f039bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "torchinfo.summary(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f532f6-16a3-454a-bbae-4b5de7514ff0",
   "metadata": {},
   "source": [
    "Again, we define an optimiser and a loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f56d24-62aa-4486-9c80-9c451166bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfe532-4541-4830-8fb0-a4da5468916e",
   "metadata": {},
   "source": [
    "And train over 10 epochs (the data is passed through the network 10 times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc894f0-b5dd-4d3d-bed7-5c6438ac251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(tqdm(trainloader), 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        #running_loss += loss.item()\n",
    "        #if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        #    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "        #    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197afa1-3e7b-41cb-94c3-7599053672a3",
   "metadata": {},
   "source": [
    "Now that we have finished training, we can test the network on the held out test set! \n",
    "\n",
    "First let's look at a batch from the test set, along with their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f45fe-d635-4e93-bb3f-a59d041fcdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = iter(testloader)\n",
    "images, labels = next(data_iterator)\n",
    "\n",
    "# print images\n",
    "imshow(make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661827b-5abf-4235-a5b0-b3eea15e9a90",
   "metadata": {},
   "source": [
    "And now check out the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346c897-95a4-4bd5-9e58-b8fc3e768ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)\n",
    "# outputs[0]\n",
    "# classes[np.argmax(outputs[0])]\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]}'\n",
    "                              for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b936dc2b-7fb0-4cf5-8f14-d4d8d9550231",
   "metadata": {},
   "source": [
    "We can get the accuracy of across all images in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbdc12d-1e22-443d-9f81-303eda105e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5930a-4735-4623-a078-8779fd1149ba",
   "metadata": {},
   "source": [
    "This is much better than random guessing, which would be around 10%. And we trained for about 1 minute, and we were using massively downsized images.\n",
    "\n",
    "## Evaluating a Neural Network\n",
    "\n",
    "To properly evaluate the network, we cannot rely on just the average accuracy above, especially when we have 10 classes or more (later we will see a 1000 class classification problem).\n",
    "\n",
    "Let's see the accuracy by class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf344d2-ba08-4e60-a509-06c2a97b458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "cr_true_labels = []\n",
    "cr_pred_labels = []\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        \n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            cr_true_labels.append(int(label))\n",
    "            cr_pred_labels.append(int(prediction))\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db55701-1f71-4927-8e1f-2d1865657353",
   "metadata": {},
   "source": [
    "We can use the `classification_report()` function from SciKit Learn that we used several times yesterday to print a report. This works even though we did not use SciKit Learn to train the model at all (as it just requires lists of predictions and labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb72b207-bbf0-4cfa-8a23-1812c244f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(cr_true_labels, cr_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad0b385-3717-4a8c-a514-1a7f7cb99611",
   "metadata": {},
   "source": [
    "And we can also plot a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8367b9-1379-41b0-99e6-cf94f4b0663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(cr_true_labels, cr_pred_labels)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot(cmap='Blues', colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c595997a-7b84-4dba-81b2-e6bb5d877872",
   "metadata": {},
   "source": [
    "Here you see the confusion across classes of the model's predictions. \n",
    "\n",
    "For example, you can quickly see tha cat has a lot of confusion. It is often confused with dog. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df197de-9f34-4b4e-88b6-cc2eb4582939",
   "metadata": {},
   "source": [
    "## Saving a Model\n",
    "\n",
    "Once you have trained a model, you can saved it to be used later. \n",
    "\n",
    "In essence what you are really doing is saving the model's weights. That means that the larger the network, the bigger the saved model. \n",
    "\n",
    "In the case of a network where each weight is some 32-bit precision number (float32), as is the generally the default, then 4 bytes are required per weight. \n",
    "\n",
    "So, for a model with 6 million parameters, we have 4 x 6 million bytes = 24 million bytes, which is 24MB. \n",
    "\n",
    "You can also see how large some of the large language models are. For example Llama from Facebook has 65 billion parameters, this is 260GB of weights for the model. Often, however, you will see 16 bit floats or even less precision, and the files sizes can be smaller, at 1 byte per parameter, which still results in 65GB file size and 65GB of GPU memory required to run the file.\n",
    "\n",
    "Luckily, our trained model is much smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86952d9-a2c8-4c04-b6cb-afae32fe556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(net, (batch_size, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd9a5f5-b114-4981-b892-3905092f2d2f",
   "metadata": {},
   "source": [
    "We have 62,006 parameters, which makes 248,024 bytes, which is only 0.25MB. As you can see there is some overhead, so the entire model is about 2.31 MB. \n",
    "\n",
    "To save the model, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9650684-0df0-493d-9b4c-ecf09fa35cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './data/cifar10-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8b898-1708-4570-a0bf-e094205a331b",
   "metadata": {},
   "source": [
    "We can have a look at the size of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f7b48-fabc-4095-ab91-892d9ccd863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lAhF ./data/cifar10-model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4ab3a6-2b61-4344-8a16-30fbc4f974c7",
   "metadata": {},
   "source": [
    "A very small model in fact! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce58f6-eaab-48fd-939b-3168faeb9d7c",
   "metadata": {},
   "source": [
    "To load a model, it is a matter of a few lines of code also:\n",
    "\n",
    "```python\n",
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "We will load a pre=trained model in an example later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f4d93-2050-4dc0-ad51-5988c1688f4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PyTorch Hub\n",
    "\n",
    "PyTorch Hub lets you download pre-trained models very easily.\n",
    "\n",
    "What is a pretrained model? This is a model that has been trained already, perhaps on millions of images, and where the weights have been made available. \n",
    "\n",
    "Using a pre-trained model saves you the effort of training a model for a long time with data you don't have. \n",
    "\n",
    "These pre-trained models can be used as is, or can be fine-tuned to suit your specific task. We will discuss fine-tuning later.\n",
    "\n",
    "Here we download ResNet18 and ask that we retrieve the pre-trained weights also. Note that is has been trained on ImagetNet, a 1,000 class classification problem. We will look at the classes soon.\n",
    "\n",
    "Download the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d01098-bf9e-4235-8a19-22fd84238528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "# Get a ResNet18 pretrained model\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "\n",
    "# Important to set the model to evaluation mode before trying to get predictions. \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b6a5a7-5ce3-4d12-a59b-912169847e92",
   "metadata": {},
   "source": [
    "Once downloaded and loaded, it will print this information.\n",
    "\n",
    "Let's make it predict an image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff6602-f97e-4bcd-8fa1-cdb3d7bf897a",
   "metadata": {},
   "source": [
    "First, we download an example image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed30674-4407-48e0-b6bb-82ef7f942dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an example image from the pytorch website\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd7ff9-abc5-4b31-9f51-968ae85c51ee",
   "metadata": {},
   "source": [
    "Then we convert this image in to a format we can view, and preview it here in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a863d73-20db-4df2-a6a0-321f0a74bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "input_image = Image.open(filename)\n",
    "input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd88f11-2b1a-43ff-b324-2a1f8cf8927e",
   "metadata": {},
   "source": [
    "Now we can ask the network to make its predictions based on the dog image above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795620e-2295-4ab5-9b5b-6596d8ec886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "#if torch.cuda.is_available():\n",
    "#    input_batch = input_batch.to('cuda')\n",
    "#    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "    \n",
    "# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n",
    "# print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2882bff-7e89-416c-82a9-1f6063f8ba70",
   "metadata": {},
   "source": [
    "As you can see, the raw output is a list of 1,000 probabilities. Each probability is a confidence score of the how likely the network thinks the image belongs to that class.\n",
    "\n",
    "These probabilities sum to 1, as we can see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ad9aa-4012-4025-b9c6-f9d1bb67682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854df0a-d64d-4b72-8341-36786499ec5b",
   "metadata": {},
   "source": [
    "Let's take the top 5 probabilities which would be the top 5 predictions of which class this image belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80963669-009a-44db-a9c4-4a21ae2a3d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the categories\n",
    "with open(\"./data/imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee3bcaf-10c8-41eb-bbfd-4e9a8c8d79d4",
   "metadata": {},
   "source": [
    "Looks like the network was right, even recognising the difference between a samoyed and a white wolf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3dc2d-a7cd-4498-8ebc-bf15b1a84497",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Medical Image Analysis\n",
    "\n",
    "In the following paper by Beck, et al. we see the following:\n",
    "\n",
    "![Patho](./img/patho-workflow.png)\n",
    "\n",
    "*Source*: Beck, A.H., Sangoi, A.R., Leung, S., Marinelli, R.J., Nielsen, T.O., Van De Vijver, M.J., West, R.B., Van De Rijn, M. and Koller, D. Systematic analysis of breast cancer morphology uncovers stromal features associated with survival. Science Translational Medicine, 3(108), 2011. <https://www.science.org/doi/10.1126/scitranslmed.3002564>\n",
    "\n",
    "What is interesting about this workflow is the amount of effort and expertise required to perform this task.\n",
    "\n",
    "Nowadays you could prob skip the vast majority of that work, and simply train a network to do this from end to end, without wortying about any of these intermediate steps. It would require none of the preprocessing and expertise and domain knowledge that was required to do the work int the paper above.\n",
    "\n",
    "We can illustrate this now by training a network to predict skin lesions types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d0a07e-376a-4d03-8413-5a9c1402b3bf",
   "metadata": {},
   "source": [
    "# MedMNIST\n",
    "\n",
    "We have seen MNIST, let's take a look at MedMNIST.\n",
    "\n",
    "MedMNIST is a collection of pre-processed medical image datasets, designed specifically for benchmarking machine learning models in the context of healthcare. To demonstrate its use, we will walk you through the process of loading some of its datasets and previewing the data using Python.\n",
    "\n",
    "Let's make a number of imports and define a function to view the images easily, as we will do this several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c678070-7d0f-4eb6-b97e-0232dfbd62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import medmnist\n",
    "from medmnist import Evaluator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Let's also define a function to visualise images easily later\n",
    "def visualise_medmnist(data, num_images=6):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        idx = random.randint(0, len(data) - 1)\n",
    "        image, label = data[idx]\n",
    "        # image = image.squeeze()  # Remove channel dimension if it exists\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f'Label: {label}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708860db-2fa6-4c20-a75f-a18d13c3b921",
   "metadata": {},
   "source": [
    "There are several datasets included in MedMNIST (see <https://medmnist.com>), covering a wide arary of medical image types:\n",
    "\n",
    "![MedMNIST](./img/MedMNIST.png)\n",
    "\n",
    "*Source*: Yang J, Shi R, Wei D, Liu Z, Zhao L, Ke B, Pfister H, Ni B. MedMNIST v2-A large-scale lightweight benchmark for 2D and 3D biomedical image classification. Nature Scientific Data. 2023 Jan 19;10(1):41.\n",
    "\n",
    "The data included in MedMNIST covers several types of medical imagery, from dermatology, to histopathology as well as 3D datasets such as the OrganMNIST3D abdominal CT dataset. \n",
    "\n",
    "Let's have a look at the Dermatology dataset.\n",
    "\n",
    "The data can be downloaded in various file sizes, which you can specify with the `size=128` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe1af3-adab-46ab-af4e-4befcc47b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnist import DermaMNIST\n",
    "\n",
    "derma_dataset = DermaMNIST(split=\"train\", download=True, size=128)  # Can be up to 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b556cd-8001-4391-9871-665a06a978cf",
   "metadata": {},
   "source": [
    "The documentation for MedMNIST is actaully not very good on the official website, however it is well documented within the package itself. \n",
    "\n",
    "Therefore, we can use `?` at any time, as we have seen several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b0ecb5-29ca-4406-8f03-ddacbe1a94ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DermaMNIST?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13572a-a959-47f4-9acf-50e8f6f5343d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's preview the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999bcc5a-eb0f-4429-a26b-58b317a2d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "derma_dataset.montage(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42dab08-b3bb-467c-8e81-18704629a871",
   "metadata": {},
   "source": [
    "Using the `visualise_medmnist` function we defined above, we can preview this data. The function prints a random 6 images from the dataset. Each time we call the function we will get another random 6 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfddf39-450c-4ddc-82e1-44e95afe6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_medmnist(derma_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb845c-f18f-499f-989b-79154252f8c0",
   "metadata": {},
   "source": [
    "First thing to notice is that we are dealing with a dataset of skin lesions that were collected using a **dermatoscope**.\n",
    "\n",
    "A dermatoscope is similar to a camera, and provides a non-invasive method to analyse skin lesions:\n",
    "\n",
    "![Dermatoscope](./img/dermatoscope.jpeg)\n",
    "\n",
    "*Source*: <https://commons.wikimedia.org/w/index.php?curid=2431174> \n",
    "\n",
    "The camera can fully enclose the lesion and therefore is unaffected by ambient light, and has its own powerful light that illuminates the lesion in a way that enhances the visibility of surface patterns and colours not seen by the naked eye. Dermatoscopes have a magnifier, typically around 10x. \n",
    "\n",
    "This close-up view provides detailed information about the morphology, skin lesions, melanomas and other types of skin cancer, moles and so on.\n",
    "\n",
    "Let's take a look at what `derma_dataset` contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea2dce-f675-43bc-bacd-7c26ce4e6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "derma_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90e3cd-83d6-4002-be41-3c882060b4a8",
   "metadata": {},
   "source": [
    "By reading the text, we can see, we have 10,015 images in total. They are already split in to a train, validation, and test sets as this is a benchmarking dataset (therefore various groups might want to compare their results using the exact same test set).\n",
    "\n",
    "So the dataset has a total of 10,015 images and they are split up as follows:\n",
    "\n",
    "| Split      | Number     |\n",
    "|------------|------------|\n",
    "| Train      | 7,007      |\n",
    "| Validation | 1,003      |\n",
    "| Test       | 2,005      |\n",
    "| **Total**  | **10,015** |\n",
    "\n",
    "The images are in colour, as indicated by the `Number of channels: 3` for the three channels of RGB.\n",
    "\n",
    "Last, the images are of 7 distinct diseases, and is therefore a multi-class classification problem.\n",
    "\n",
    "The 7 diseases are shown in the `Meaning of labels` field and are as follows: \n",
    "\n",
    "0. actinic keratoses and intraepithelial carcinoma\n",
    "1. basal cell carcinoma\n",
    "2. benign keratosis-like lesions\n",
    "3. dermatofibroma\n",
    "4. melanoma\n",
    "5. melanocytic nevi\n",
    "6. vascular lesions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b09b7f-6ba3-4d84-a5a5-fabca6a99b14",
   "metadata": {},
   "source": [
    "Maybe it would be interesting to see how each of the 7 different classes look. For example, are the melanoma vastly different in appearance to nevi? \n",
    "\n",
    "Note here that melanocytic nevi or nevi are benign lesions normally called moles, while melanoma are cancerous, malignant lesions that are harmful and can spread (metastasise). \n",
    "\n",
    "We can also take a look at how each class looks.\n",
    "\n",
    "We can use `np.where()` to search for subsets of data by their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c7af1-9d79-431d-9647-662ccd90577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['keratoses', 'basal', 'benign keratosis', 'dermatofibroma', 'melanoma', 'nevi', 'vascular']\n",
    "\n",
    "keratoses_idx = np.where(derma_dataset.labels==0)[0]\n",
    "basal_idx = np.where(derma_dataset.labels==1)[0]\n",
    "benign_keratoses_idx = np.where(derma_dataset.labels==2)[0]\n",
    "dermatofibroma_idx = np.where(derma_dataset.labels==3)[0]\n",
    "melanoma_idx = np.where(derma_dataset.labels==4)[0]\n",
    "nevi_idx = np.where(derma_dataset.labels==5)[0]\n",
    "vascular_idx = np.where(derma_dataset.labels==6)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0d926-f5c4-4017-bd4f-86fcde9e0258",
   "metadata": {},
   "source": [
    "Now we can preview each class to see if something interesting is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6ff7f-b170-44ab-976c-37a191123878",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_medmnist([(derma_dataset.imgs[i], \"Keratosis\") for i in keratoses_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3d885-65fb-4e40-80d0-dccbf2437340",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_medmnist([(derma_dataset.imgs[i], \"Melanoma\") for i in melanoma_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113707c-799e-4390-9e8b-31c95f5492f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_medmnist([(derma_dataset.imgs[i], \"Nevi\") for i in nevi_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e24200-0b98-4e4f-b0fb-1cdd7adf9375",
   "metadata": {},
   "source": [
    "## ABCDE\n",
    "\n",
    "If we think back to how we might tackle the problem of classifying or diagnosing skin lesions based on features, it might make sense to understand how a dermatologist or doctor might make such a diagnosis. \n",
    "\n",
    "Upon visual examination by dermatologist, the following characteristics are normally noted (ABCDE test):\n",
    "\n",
    "- A: Asymmetry - melanoma often present as asymetrical and non-uniform, non-round. \n",
    "- B: Border - melanoma have less well-defined borders than non-cancerous moles. They appear to smudge between skin tissue and the lesion itself.\n",
    "- C: Colour - melanoma are darker in appearance (melanoma start in the melanocytes, and these are the cells that give your skin its pigment) and have multiple shades of colours, while non-cancerous moles tend to have one colour\n",
    "- D: Diameter - melanoma are larger and will grow larger than 5 or 6mm. \n",
    "- E: Evolution - melanoma tend to change shape, size, and colour oven time, or nayb begin to itch or bleed, unlike benign moles.\n",
    "\n",
    "See: <https://www.mayoclinic.org/diseases-conditions/melanoma/symptoms-causes/syc-20374884>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e596d96-6a65-4c8e-af65-2cfd184861a2",
   "metadata": {},
   "source": [
    "## Closer Look at the Data\n",
    "\n",
    "Let's take a closer look at the data and determine how to train a network with it.\n",
    "\n",
    "For example, we can take a look at the distribution of the various classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79366906-824e-415d-8eee-589f7c53cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the unique values and their counts/frequencies\n",
    "unique, counts = np.unique(derma_dataset.labels, return_counts=True)\n",
    "\n",
    "# Print nicely using a DataFrame\n",
    "pd.DataFrame(np.asarray((unique, counts)).T, columns=['class id', 'freq.'], index=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ee2f2-4dc6-415c-baa6-3c4d90857cf3",
   "metadata": {},
   "source": [
    "Notice the very large imbalance!\n",
    "\n",
    "We will see how this might be an issue later.\n",
    "\n",
    "First though, let's train a network.\n",
    "\n",
    "First, define some values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6aa2d3-78b4-4137-a1ce-f034737767fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 3\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "\n",
    "# task = 'multi-task'\n",
    "number_of_channels = 3\n",
    "number_of_classes = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cdc60e-332a-42d8-9fce-5c699bafdee5",
   "metadata": {},
   "source": [
    "Load the relevant Torch libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24cfaa-6ed0-4e46-be4f-7671343906af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "# from torchvision.transforms import v2 as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67493fd-586c-4136-ad2a-6f04b82270b5",
   "metadata": {},
   "source": [
    "Apply transforms, including resizing the images to 28x28:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70680ed-5d01-41f9-981d-4c0189c0d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomResizedCrop(size=(28, 28), antialias=True),\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #transforms.RandomInvert(p=0.5),\n",
    "    # transforms.Normalize(mean=[.5], std=[.5])\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ccc72-a322-4676-8527-73c1c9712965",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DermaMNIST(split='train', transform=data_transform, download=True)\n",
    "test_dataset = DermaMNIST(split='test', transform=data_transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa76ad-d27c-4250-ab82-66d3a09542a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a7a8d-4fc3-44c4-bd9a-7c0e646411b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple CNN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = Net(in_channels=number_of_channels, num_classes=number_of_classes)\n",
    "    \n",
    "# define loss function and optimizer\n",
    "#if task == \"multi-label, binary-class\":\n",
    "#    criterion = nn.BCEWithLogitsLoss()\n",
    "#else:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226bdd1-a7a3-4020-b08a-de33d4f90f04",
   "metadata": {},
   "source": [
    "We can check out the model's structure here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da93faa-4211-4e78-8671-9600bc2ae2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb128e9-7153-4189-b4f1-8a8f14527535",
   "metadata": {},
   "source": [
    "And now do the actual training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e1953-3fbe-4822-bdac-69d261479b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    model.train()\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        # forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        #if task == 'multi-label, binary-class':\n",
    "        #    targets = targets.to(torch.float32)\n",
    "        #    loss = criterion(outputs, targets)\n",
    "        #else:\n",
    "        targets = targets.squeeze().long()\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c49015-75a6-45cb-a83c-86dd94b6e34f",
   "metadata": {},
   "source": [
    "Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be09f5-a88a-4c1f-8975-5fe8cf49bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_cr = []\n",
    "y_true_cr = []\n",
    "\n",
    "def test(split):\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([])\n",
    "    y_score = torch.tensor([])\n",
    "    \n",
    "    data_loader = train_loader if split == 'train' else test_loader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            #if task == 'multi-label, binary-class':\n",
    "            #    targets = targets.to(torch.float32)\n",
    "            #    outputs = outputs.softmax(dim=-1)\n",
    "            #else:\n",
    "            targets = targets.squeeze().long()\n",
    "            outputs = outputs.softmax(dim=-1)\n",
    "            targets = targets.float().resize_(len(targets), 1)\n",
    "\n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, outputs), 0)\n",
    "\n",
    "        \n",
    "        y_true = y_true.numpy()\n",
    "        y_score = y_score.detach().numpy()\n",
    "        \n",
    "        evaluator = Evaluator('dermamnist', split)\n",
    "        metrics = evaluator.evaluate(y_score)\n",
    "    \n",
    "        # print('%s  auc: %.3f  acc:%.3f' % (split, *metrics))\n",
    "        print(f\"Model accuracy: {metrics[1]:.3f}\")\n",
    "        \n",
    "        # For classification report\n",
    "        for score in y_score:\n",
    "            y_pred_cr.append(np.argmax(score))\n",
    "\n",
    "        for score in y_true:\n",
    "            y_true_cr.append(int(score[0]))\n",
    "        \n",
    "#test('train')\n",
    "test('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd93b27b-c0ff-424b-b137-556e01ef052a",
   "metadata": {},
   "source": [
    "At first, this looks pretty good. \n",
    "\n",
    "We have an accuracy of 67%... let's break down the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255eec7-cbb4-47e2-b8b1-92968d015e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true_cr, y_pred_cr, zero_division=0.0, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f693fbb1-7958-458f-b86a-3ff44eb2600a",
   "metadata": {},
   "source": [
    "Not good results! \n",
    "\n",
    "Let's try a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe46c1-ede5-4c95-86b6-65d744094370",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true_cr, y_pred_cr)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap='Blues', colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac15124-6138-46cf-874a-5f0c7982c0f1",
   "metadata": {},
   "source": [
    "That is not very good. \n",
    "\n",
    "How could this be fixed? \n",
    "\n",
    "What are the various approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9816e27-af83-4035-ab56-bf598db66fd2",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "Fine-tuning is the process by which you update or fine-tune a pretrained network to match your particular task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae444d-9569-41c9-821d-1528296b0a02",
   "metadata": {},
   "source": [
    "# Image Augmentation\n",
    "\n",
    "Image augmentation is use frequently in deep learning.\n",
    "\n",
    "Let's take a look at Augmentor to see what augmenation is: <https://github.com/mdbloice/Augmentor>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0a744-abdc-46d2-b555-4e5a40b2377e",
   "metadata": {},
   "source": [
    "# Distributing Your Work\n",
    "\n",
    "There are several reasons why you might want to make a model you have trained available.\n",
    "\n",
    "## Publishing / Distributing Models\n",
    "\n",
    "Once you have trained a model, and you wish to publish it, what are your options?\n",
    "\n",
    "First, by publishing, what do we mean. For example, let's say you were working on a dataset here at the University for a department. You create a model based on the data they have provided for you, and you would like to use this model in an application that the doctors can use. \n",
    "\n",
    "In this case, how would you create an application for them to use? \n",
    "\n",
    "You could create a tradtional application, perhaps with Java, and then install this on their machines. \n",
    "\n",
    "This is problematic for several reasons... \n",
    "\n",
    "A better approach might be to create a web application. Some advantages are:\n",
    "\n",
    "- You only need to provide a URL to the end users\n",
    "- The web application can run internally on the network and not be on the internet\n",
    "- Updates that you make are instantly distributed\n",
    "- No installation required by the end user, no issues with administation rights\n",
    "- Multi-platform: any device with a web browser can access your application\n",
    "\n",
    "Let's see how we might do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8f1ad-5434-438d-bf37-58005177971d",
   "metadata": {},
   "source": [
    "## Shiny\n",
    "\n",
    "Shiny is probably the gold standard for creating web applications quickly and easily. However... Shiny is developed for R and not for Python. \n",
    "\n",
    "While Shiny might be the best framework to get going with developing a data-centric web application, `R` is not really perfectly suitable for training deep models. Python dominates this field. Therefore, if you were wishing to use some deep models, then R is not really the most suitable language to use. \n",
    "\n",
    "If however, you were wishing to provide a new web-based interface for your Python project, it is still possible to use \n",
    "\n",
    "However, it is possible to import and run a model that you have trained using Python\n",
    "\n",
    "For the demonstration below, I used Python along with the Keras framework, to \n",
    "\n",
    "![Patho1](./img/fed1.png)\n",
    "\n",
    "Ignore that fact that the images are MNIST digits! \n",
    "\n",
    "\n",
    "The user was required to label these images, and in the background the model is being trained. \n",
    "\n",
    "In order to get this up and running, we can load a model that was trained in Python, and read it with \n",
    "\n",
    "In this case, I created and trained a model using Keras, which is a high-level library for TensorFlow. We will not cover TensorFlow in this course. The main reason for this is that PyTorch is used more often, and is preferred in academia and research. However, interestingly, Keras now supports PyTorch as a back-end, since version 3, see <https://keras.io/keras_3/> if you are interested. \n",
    "\n",
    "Keras provides a package for R, and loading a model that\n",
    "\n",
    "```R\n",
    "library(keras)\n",
    "\n",
    "model <- load_model_hdf5(\"./vgg19finetuned.h5\")\n",
    "\n",
    "```\n",
    "\n",
    "You can now access the model and use `predict_classes()`:\n",
    "\n",
    "```R\n",
    "predictions <- predict_classes(im)\n",
    "```\n",
    "\n",
    "to make predictions based on a network that you trained using Python. \n",
    "\n",
    "See <https://cran.r-project.org/web/packages/keras/vignettes/>\n",
    "\n",
    "Combining R and Python in this way might be a good solution if you prefer to make your web applications using R. I would argue that Shiny is the easiest way to create proper web applications that can be distributed to other users, e.g. on the hospital network. \n",
    "\n",
    "![Patho1](./img/fed2.png)\n",
    "\n",
    "Later we will see how to build a web interface for a model that we have previously trained using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6463eb-6ffa-41b9-b164-f5e6c3507b33",
   "metadata": {},
   "source": [
    "## StreamLit\n",
    "\n",
    "Now, because the focus of this section of the course is of course Python, we will cover a Python equivalent of Shiny for Python. Several such packages exist, but we shall talk about StreamLit. \n",
    "\n",
    "In this section will show how to use StreamLit and demo it for running a pre-trained model. \n",
    "\n",
    "## Basics\n",
    "\n",
    "Streamlit allows you to build web applications using pure Python. It is similar to Shiny. \n",
    "\n",
    "Streamlit provides a number of controls, such as buttons, sliders, and so on so that you can interact with the application and the data. \n",
    "\n",
    "To start a Streamlit app you just run:\n",
    "\n",
    "```sh\n",
    "$ streamlit run script.py\n",
    "```\n",
    "\n",
    "from the command line. \n",
    "\n",
    "Let's write a very simple app now. We will place the following code in a file called `slider.py`:\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "\n",
    "# Write a title and some text to the app:\n",
    "st.title('Slider App')\n",
    "st.write('Demo of Streamlit Interaction')\n",
    "\n",
    "# Create a slider widget:\n",
    "value = st.slider('Please select a value:', 0, 100, 50)\n",
    "\n",
    "# Write the value back to the user:\n",
    "st.write(\"You selected the value of \", value, ' using the slider.')\n",
    "```\n",
    "\n",
    "We use the element `slider()` element to display the slider, with a minimum of 0, a maximum value of 100, and a default value of 50.\n",
    "\n",
    "Also notice, it is convention to import Streamlit as `st`, which gives you access to all the Streamlit controls.\n",
    "\n",
    ">See all the controls that Streamlit has available here: <https://docs.streamlit.io/library/api-reference> and there is also a useful Cheat Sheet: <https://docs.streamlit.io/library/cheatsheet> \n",
    ">\n",
    ">On the left hand side, you will see all the types of elements, such as layout elements and data elements, and so on.\n",
    "\n",
    "Of course, we can apply functions to the data, so we can modify this script to show your selected value squared, by adding the following function to the script:\n",
    "\n",
    "```python\n",
    "def squared(x):\n",
    "    return x**2\n",
    "```\n",
    "\n",
    "and then modify the `st.write()` statement to say the following: \n",
    "\n",
    "```python\n",
    "st.write(\"I'm \", squared(x), \"years old, squared.\")`\n",
    "```\n",
    "\n",
    "or something similar.\n",
    "\n",
    "Notice how when we made a change to the source code, the web application noticed this and asked if we'd like to reload the script. \n",
    "\n",
    "### Design principles\n",
    "\n",
    "But how does Streamlit update the `value` in the script above? How does it know when to update it? Normally in JavaScript you'd need to write all these event handlers to do this, for example by manually writing `onClick(e)` event handlers that are executed when events occur. This is quite complicated and requires a lot of coding and knowledge of HTML, JavaScript, and how web applications work.\n",
    "\n",
    "Streamlit works in the following way: when you start an application, the `streamlit` application watches your web application, and any change to any element, such as a slider, re-runs your entire script again. You have to keep this in mind, that the entire script is re-run. \n",
    "\n",
    "This might sound time consuming and slow, especially if there is a calculation that is performed when a user moves a slider, but you can cache results so that a calculation only needs to be run once. If a function that is executed when an element is changed, you can use the `@st.cache_data` decorator to tell Streamlit to cache the results.\n",
    "\n",
    "#### Displaying Data\n",
    "Streamlit is focussed on data science and machine learning applications. Therefore the displaying of data is one of its strengths. \n",
    "\n",
    "Consider the following application:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "# Displaying Data\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "  'first column': [1, 2, 3, 4],\n",
    "  'second column': [10, 20, 30, 40]\n",
    "})\n",
    "\n",
    "df\n",
    "```\n",
    "\n",
    "Let's save this to a file called `table.py` and run it using `streamlit run table.py` from the command line.\n",
    "\n",
    "As you can see, you get a lot of interactivity for free. You can search rows, expand cells by double clicking on them, and so on.\n",
    "\n",
    "If you are in doubt how to display an element, just try `st.write()` and most likely it will work.\n",
    "\n",
    "Also, plotting is supported, much like in `R`/`Shiny`. For example the following draws a simple plot:\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "chart_data = pd.DataFrame(\n",
    "     np.random.randn(20, 3),\n",
    "     columns=['a', 'b', 'c'])\n",
    "\n",
    "st.line_chart(chart_data)\n",
    "```\n",
    "\n",
    "We can save this to `plot.py` and see its output.\n",
    "\n",
    "\n",
    "Note, that if you wanted to draw a plot, you could generate the plot using MatPlotLib or Seaborn, and just use `st.write()` and it will most likely work. \n",
    "\n",
    "For example, for a MatPlotLib plot we could say the following:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "arr = np.random.normal(1, 1, size=100)\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(arr, bins=20)\n",
    "\n",
    "st.write(fig)\n",
    "```\n",
    "\n",
    "Note that there is actually a `pyplot()` function, so you would probably want to use `st.pyplot(fig)` instead of `st.write(fig)` but this demonstrates the fleixibility of the `st.write()` function.\n",
    "\n",
    "So, those are the basics of how Streamlit applications work: you write a Python script, and Streamlit handles the updating of the elements by re-running the script each time the user makes a change to any button, slider, and so on. Of course, in our case, we wish to integrate a trained model with a web application, and we will do this now.\n",
    "\n",
    "## Integtration With a Trained Model\n",
    "\n",
    "For any models that you might train during your PhD, or you work at the medical university, you will want to be able to use Streamlit to actually deploy a machine learning based application. This can take several forms, for example:\n",
    "\n",
    "- An application that allows an expert to label images, such as the example above. This application could also train a model as \n",
    "- An application that allows interaction with a trained model, for example one that allows a doctor to query the network for a classification based on an image he or she might upload\n",
    "\n",
    "In the first scenario you are using the application to train your model and to collect data. In the second scenario you are using a trained model that has you have already prepared.\n",
    "\n",
    "Step one, let's create a new file, let's call it 'model.py' and add the following code to it:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "\n",
    "# Set title of app\n",
    "st.title(\"Image Classifier)\n",
    "st.write(\"Demo application for the ATSP course\")\n",
    "\n",
    "# Enable users to upload images for the model to make predictions\n",
    "uploaded_file = st.file_uploader(\"Upload your image\", type = \"jpg\")\n",
    "```\n",
    "\n",
    "If we run this we see we have a nice file uplaoder, which let's use choose a file and upload it to the webpage. The `st.file_uploader()` function is doing all the hard work, including telling the browser that only JPEG images are allowed, that there is a 200MB limit, and also returns the uploaded image to the script in the form of `uploaded_file`. \n",
    "\n",
    "Also, as we can see we now need to import Torch and Torch Vision, as we are going to load a model previously trained using Torch. \n",
    "\n",
    "If we wanted to display this file back to the user, it is a simple case of using `st.image()` (note, you can add code to a running app, and Streamlit will notice a change and ask yo if you want to rerun the application):\n",
    "\n",
    "```python\n",
    "if uploaded_file is not None:\n",
    "    # Display image that user uploaded\n",
    "    image = Image.open(uploaded_file)\n",
    "    st.image(image, caption = 'Uploaded Image', use_column_width = True)\n",
    "    st.write(\"\")\n",
    "```\n",
    "\n",
    "Notice also that we included this `if` statement. What we are doing is checking if an image exists. If it exists, that is if the user uploaded the image, then display it. Otherwise display nothing. \n",
    "\n",
    "Remember, every time a user interacts with an element, the entire script is re-run. Meaning after the user uploads a file, the script is re-run, this time with an image contained in `uploaded_file` and then the script above will execute.\n",
    "\n",
    "Now that we have our image, we can load up a model and pass the image to the model. For that we will define a function that will be called when a user uploads an image. Let's just call it `predict()`: \n",
    "\n",
    "```python\n",
    "def predict(image):\n",
    "    # Return top 5 predictions ranked by highest probability.\n",
    "    # First create a ResNet model\n",
    "    resnet = models.resnet101(pretrained = True)\n",
    "\n",
    "    # Transform the input image through resizing, etc. We have seen this several times in the course already.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean = [0.485, 0.456, 0.406],\n",
    "            std = [0.229, 0.224, 0.225]\n",
    "        )])\n",
    "\n",
    "    # Load the image, apply preprocessing, and make predictions\n",
    "    img = Image.open(image)\n",
    "    single_image_batch = torch.unsqueeze(transform(img), 0)\n",
    "    resnet.eval()\n",
    "    out = resnet(single_image_batch)\n",
    "\n",
    "    # Read the classes file, which contains the 1000 classes of ImageNet\n",
    "    with open('imagenet_classes.txt') as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    # Return the top 5 predictions, ranked by probability\n",
    "    prob = torch.nn.functional.softmax(out, dim = 1)[0]\n",
    "    _, indices = torch.sort(out, descending = True)\n",
    "\n",
    "    return [(classes[i], prob[i].item()) for i in indices[0][:5]]\n",
    "```\n",
    "\n",
    "We can place this anywhere in the code, as it will be executed by Streamlit when the app is loaded.\n",
    "\n",
    "Now we need to display these predictions, which we will do under the image, where we said `if uploaded_file is not None:` previously:\n",
    "\n",
    "```python\n",
    "if uploaded_file is not None:\n",
    "\n",
    "    ...\n",
    "    \n",
    "    # Make the prediction\n",
    "    labels = predict(uploaded_file)\n",
    "    \n",
    "    # Format the labels a bit better\n",
    "    formatted = []\n",
    "    for i, v in enumerate(labels):\n",
    "        formatted.append([i+1, v[0].split(',')[1], v[0].split(',')[0], v[1]])\n",
    "    \n",
    "    # Save to a Pandas DataFrame\n",
    "    df = pd.DataFrame(formatted, columns=['Rank', 'Prediction', 'Class Index', 'Probability'])\n",
    "    df = df.set_index('Rank')\n",
    "    \n",
    "    # Show the DataFrame\n",
    "    st.dataframe(df)\n",
    "```\n",
    "\n",
    "Now we can see the results in a Pandas DataFrame. If we wanted to make the table match the size of the parent element, we could use `st.dataframe(df, use_container_width=True)`. \n",
    "\n",
    "---\n",
    "\n",
    "We should think back to our comic that we saw earlier today:\n",
    "\n",
    "<img src=\"./img/task.png\" width=\"400\"/>\n",
    "\n",
    "*Source*: <https://xkcd.com/1425/>\n",
    "\n",
    "And notice that we have come a long way :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ae115-e1d2-4d26-a645-be8fc64a09ab",
   "metadata": {},
   "source": [
    "# End of Morning Session\n",
    "\n",
    "In the next session, we will discuss your assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf610e-55ac-4822-8fb5-e89b3e3e00ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-course",
   "language": "python",
   "name": "python-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
