{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03d6320-add2-4d84-a6c6-91a70e84272e",
   "metadata": {},
   "source": [
    "# Assignment \n",
    "\n",
    "As part of this course, you should complete an assignment. \n",
    "\n",
    "You have the choice of one of three different assignments.\n",
    "\n",
    "Here are your choices, choose **one** of the following: \n",
    "\n",
    "1. Medical Image Classification\n",
    "2. Streamlit web application\n",
    "3. Tabular Data Classification or Regression\n",
    "\n",
    "## Dealine\n",
    "\n",
    "**Deadline is the 25th of March!!**\n",
    "\n",
    "**Note**: Send file(s) in a Zip file, as the Med Uni Graz spam filter will block files ending in `.py` for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89597b7-3359-4b23-8b73-2e7073b65071",
   "metadata": {},
   "source": [
    "## 1. Medical Image Classificaiton\n",
    "\n",
    "In this task, you are expected to classify dermatology medical images. \n",
    "\n",
    "For this, we will use MedMNIST, specifically DermaMNIST. The best approach would be to train on larger images, either 128x128 or even larger sized images. In the example during the seminar, we used the reduced size dataset. This resulted in poor performance. The bigger the image you choose, the better it will work.\n",
    "\n",
    "Options:\n",
    "\n",
    "1. Use larger images (as large as possible, but at least 128)\n",
    "2. Resampling to re-balance the dataset during training\n",
    "3. Use a Pre-trained Network (PyTorch Hub), for example ResNet. ResNet is available in difference depths, with ResNet18 being the smallest. The deeper the network, the more memory you will need and the longer it will require to train, but potentially the deeper the network the better the performance.\n",
    "4. Augmentation (Use the PyTorch `transforms.Compose` code from the seminar notebooks)\n",
    "5. Change from 7 class to 2 class (malignant/benign): prob use this as a last resort! \n",
    "\n",
    "The best and easiest solution might be to train a network using the full sized images. Here are the available image sizes for DermaMNIST:\n",
    "\n",
    "```python\n",
    "from medmnist import DermaMNIST\n",
    "\n",
    "DermaMNIST.available_sizes\n",
    "```\n",
    "\n",
    "returns the following: `[28, 64, 128, 224]`.\n",
    "\n",
    "The size 224x224 is common among pre-trained networks as this was a size often used by competitions over the years.\n",
    "\n",
    "For example, ResNet18 has been pre-trained on ImageNet, and these are 224x224 images. Hence, using PyTorch Hub to get a pretrained ResNet18 is a quick way to get going:\n",
    "\n",
    "```python\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "```\n",
    "\n",
    "However, ImageNet is a 1,000 class classification dataset, and therefore ResNet18 will have 1,000 output neurons, while DermaMNIST has 7 classes. You will need to adjust the output layer to have 7 output neurons instead of 1,000.\n",
    "\n",
    "This can be done using the following syntax:\n",
    "\n",
    "```python\n",
    "num_classes = 7\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "```\n",
    "\n",
    "### Colab\n",
    "\n",
    "If you want to work on the larger images, you might need to use a GPU, especially if you decide to use some of the very deep ResNet models. You can get a free GPU from Google Colab:\n",
    "\n",
    "<https://colab.research.google.com/>\n",
    "\n",
    "Change the runtime to GPU: Click Runtime -> Change runtime type and select T4 GPU.\n",
    "\n",
    "**Google Colab users**: If you encounter issues with PyTorch complaining about a missing GPU device with the example code provided in the 'Day 2 - Morning.ipynb' notebook, you can run the following after you have imported Torch:\n",
    "\n",
    "```python\n",
    "import torch \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "```\n",
    "\n",
    "### ResNet Sizes\n",
    "\n",
    "ResNet is available in several depths:\n",
    "\n",
    "```python\n",
    "model = models.resnet18(pretrained=True)\n",
    "model = models.resnet34(pretrained=True)\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = models.resnet101(pretrained=True)\n",
    "model = models.resnet152(pretrained=True)\n",
    "```\n",
    "\n",
    "The larger the network, the longer it will require to train and the memory it will require. However, larger networks may improve performance.\n",
    "\n",
    "### Fine Tuning\n",
    "\n",
    "If you wish to **fine-tune** a pre-trained network, the following code can be used:\n",
    "\n",
    "```python\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "```\n",
    "\n",
    "Fine tuning 'freezes' most of the pre-trained network's layers so that they are not updating during fine-tuning. Therefore fine-tuning is the act of training a pre-trained network, where you only update the final layer or final several layers during training. The early layers of the network are frozen in that case.\n",
    "\n",
    "The code above should be placed before you adjust the final output layer to 7 neurons for the 7 output classes of DermaMNIST.\n",
    "\n",
    "Fine tuning normally is quite a bit faster than training the entire network, as only the final layer's weights are updated during training.\n",
    "\n",
    "### Submission\n",
    "Send a Jupyter notebook with the accuracy, classification report, confusion matrix, etc (please send a `.zip` file as some code files are blocked by the spam filter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16095e-fb47-4b25-add1-e0459d4df238",
   "metadata": {},
   "source": [
    "## 2. Streamlit Web Application\n",
    "\n",
    "For this assignment, you must create a Streamlit application. The web app should let a user enter some details about the house, and it should predict the house price. \n",
    "\n",
    "A model is provided here as [price_model.pickle](https://github.com/imigraz/ATSP-2024/blob/master/Python-Content/price_model.pickle) in this directory. \n",
    "\n",
    "### Training Your Own Model\n",
    "\n",
    "**If you wish to train your own model**:\n",
    "\n",
    "- You must train a model on some data about house prices.\n",
    "- Here is a house price dataset: <https://www.kaggle.com/datasets/yasserh/housing-prices-dataset>\n",
    "- Use only 3 or 4 of the features, e.g. area, number of bedrooms, and number of bathrooms. \n",
    "\n",
    "### Web Application Interface\n",
    "\n",
    "The web app should contain 3 fields (area, number of bedrooms, number of bathrooms), and send the values to the model if all 3 fields are completed.\n",
    "\n",
    "Use something like \n",
    "\n",
    "```python\n",
    "text1 = st.text_input(label=\"Area Square Meters\")\n",
    "```\n",
    "\n",
    "to create text fields.\n",
    "\n",
    "**Note**: the provided model requires that the **minimum number of bedrooms is 1** and the **minimum number of bathrooms is 1**, and that the **area is not 0**, or some very low and unrealistic number. The model will return negative or nonsense numbers if you have 0 bedrooms or 0 bathrooms. \n",
    "\n",
    "You can control for user input in the Streamlit application. For example, you can check if all the fields (area, etc.) are filled in **and** check if number of bedrooms or number of bathrooms are all greater than 1 before sending the data to the model.\n",
    "\n",
    "### Loading a Model\n",
    "\n",
    "You can load a *pickled* model using:\n",
    "\n",
    "```python\n",
    "import pickle \n",
    "\n",
    "model_from_file = pickle.load(open('price_model.pickle', 'rb'))\n",
    "\n",
    "predicted_price = model_from_file.predict([[250, 2, 3]])  # Notice the double [] brackets!\n",
    "```\n",
    "\n",
    "This will return the prediction of the house price given 250 for area, 2 for num of bedrooms, and 3 for number of bathrooms.\n",
    "\n",
    "### Submission\n",
    "\n",
    "Python file with the Streamlit application code (please send a `.zip` file as `.py` files are blocked by the spam filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85423498-efb5-4063-bd57-c91652454ae4",
   "metadata": {},
   "source": [
    "## 3. Tabular Data (Classification or Regression)\n",
    "\n",
    "In this assignment, you are free to choose any tabular dataset, and train any machine learning algorithm on this data. It can be classification or regression. Do not use any of the datasets that come with Sci-Kit Learn, use a dataset you had to download, either from OpenML or a website such as Kaggle: <https://www.kaggle.com/datasets>\n",
    "\n",
    "1. Choose an appropriate algorithm for the task\n",
    "2. Split your data properly using a train/test split\n",
    "3. Analyse the results\n",
    "4. Discuss the metrics\n",
    "5. Plot a confusion matrix\n",
    "\n",
    "Submission: Jupyter notebook (please send a `.zip` file as some code files are blocked by the spam filter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c82bd-be44-4f9e-858f-bc27e66ec5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
